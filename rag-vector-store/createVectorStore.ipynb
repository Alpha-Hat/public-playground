{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import os\n",
    "import openai\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import DeepLake \n",
    "from langchain_core.documents import Document \n",
    "from langchain_openai import OpenAIEmbeddings \n",
    "from langchain.retrievers.document_compressors import EmbeddingsFilter \n",
    "from langchain.retrievers import ContextualCompressionRetriever \n",
    "import requests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign Parameters\n",
    "DEEP_LAKE_PATH = \"s3://ragdata\" #Path to S3 Bucket\n",
    "PDF_FILE_PATH = \"/path/to/your/file.pdf\" #Add local file path to the PDF with your RAG data\n",
    "OPENAI_API_KEY = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" #Add your OpenAI API Key for choosing embedding model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RAG data \n",
    "try:\n",
    "    \n",
    "    pdf_loader = PyPDFLoader(PDF_FILE_PATH)  \n",
    "    documents = pdf_loader.load()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading PDF: {e}\")\n",
    "    raise Exception(\"Error: failed to load RAG file\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed data to vector store\n",
    "\n",
    "embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY, model=\"text-embedding-3-small\") \n",
    "vector_store = DeepLake(dataset_path=DEEP_LAKE_PATH, embedding=embeddings)\n",
    "\n",
    "vector_store.add_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tailor document output \n",
    "def pretty_print_docs(docs):\n",
    "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke basic retrieval across all documents\n",
    "\n",
    "#embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY, model=\"text-embedding-3-small\") \n",
    "#vector_store = DeepLake(dataset_path=DEEP_LAKE_PATH, embedding_function=embeddings)\n",
    "#retriever = vector_store.as_retriever()\n",
    "#docs = retriever.invoke(\"How should I approach investing my 100000 dollars for retirement if I am 40 years old and risk adverse?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized retrieval using a LLM search over documents\n",
    "\n",
    "#from langchain.retrievers import ContextualCompressionRetriever\n",
    "#from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "#from langchain_openai import OpenAI\n",
    "\n",
    "#choose scanner\n",
    "#llm = OpenAI(api_key=OPENAI_API_KEY, temperature=0)\n",
    "#choose extractor\n",
    "#compressor = LLMChainExtractor.from_llm(llm)\n",
    "# initialize retriever\n",
    "#retriever = vector_store.as_retriever()\n",
    "#compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever)\n",
    "\n",
    "#compressed_docs = compression_retriever.invoke(\"How should I approach investing my 100000 dollars for retirement if I am 40 years old and risk adverse?\")\n",
    "#pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized (AND LOWER COST) retrieval using a filter based on embeddings similarities and then a more targeted LLM search over documents\n",
    "# https://python.langchain.com/docs/how_to/contextual_compression/\n",
    "\n",
    "\n",
    "# choose embedding model for query\n",
    "embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY, model=\"text-embedding-3-small\") \n",
    "# initialize vector store and retriever\n",
    "vector_store = DeepLake(dataset_path=DEEP_LAKE_PATH, embedding=embeddings)\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# set embedding similarity threshold to determine what information gets ignored (lower threshold = more relaxed constraint)\n",
    "embeddings_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.3)\n",
    "# initialize retriever based on filter settings and embedding settings \n",
    "compression_retriever = ContextualCompressionRetriever(base_compressor=embeddings_filter, base_retriever=retriever)\n",
    "\n",
    "# pass the filtering query\n",
    "compressed_docs = compression_retriever.invoke(\"How should I approach investing my 100000 dollars for retirement if I am 40 years old and risk adverse?\")\n",
    "# print queried results \n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to combine RAG, prompt engineering, and user request\n",
    "def get_combined_response(user_query, compressed_docs, character):\n",
    "    # Retrieve relevant documents based on the user query\n",
    "    compressed_docs = compressed_docs\n",
    "    character = character\n",
    "    # Combine documents into a single text\n",
    "    combined_docs_text = \"\\n\".join([doc.page_content for doc in compressed_docs])\n",
    "    \n",
    "    # Combine user query and retrieved documents\n",
    "    prompt = f\"Who you are: {character}\\n\\nUser question: {user_query}\\n\\nContext from documents:\\n{combined_docs_text}\"\n",
    "    \n",
    "\n",
    "    return prompt\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the RAG Vector-Store Implementation - the below would be added to a Lambda function for production. \n",
    "\n",
    "# initiate bedrock\n",
    "bedrock = boto3.client(service_name='bedrock', region_name='us-east-1')\n",
    "bedrock_runtime = boto3.client(service_name='bedrock-runtime', region_name='us-east-1')\n",
    "\n",
    "# run combined_response to bring all requests and context into the prompt\n",
    "prompt = get_combined_response(user_query=\"How should I approach investing my 100000 dollars for retirement if I am 40 years old and risk adverse?\", \n",
    "                               compressed_docs=compressed_docs, \n",
    "                               character=\"you are a financial advisor helping a user make financial decisions that benefit them\")\n",
    "\n",
    "# assign API inputs\n",
    "inputs = json.dumps({\n",
    "    \"prompt\": \"\\n\\nHuman: \"+ prompt + \"\\n\\nAssistant:\", \n",
    "    \"temperature\": 0.7, \n",
    "    \"top_p\": 0.901, \n",
    "    \"top_k\":250, \n",
    "    \"max_tokens_to_sample\": 3000, \n",
    "    \"stop_sequences\": [\"\\n\\nHuman:\"], \n",
    "    \"anthropic_version\": 'bedrock-2023-05-31'})\n",
    "modelId = 'anthropic.claude-v2'\n",
    "accept = 'application/json'\n",
    "contentType = 'application/json'\n",
    "\n",
    "# invoke bedrock API and call response\n",
    "response = bedrock_runtime.invoke_model(body=inputs, modelId=modelId, accept=accept, contentType=contentType)\n",
    "response_body = json.loads(response.get('body').read())\n",
    "result = response_body['completion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print response\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
