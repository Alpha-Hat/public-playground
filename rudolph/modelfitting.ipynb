{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, MultiHeadAttention, LayerNormalization, Add\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching story: The Boy And The Filberts from /author/aesop/short-story/the-boy-and-the-filberts/...\n",
      "https://americanliterature.com/author/aesop/short-story/the-boy-and-the-filberts/\n",
      "Story text extracted successfully:\n",
      "A Boy was given permission to put his hand into a pitcher to get some filberts. But he took such a great fistful that he could not draw his hand out again. There he stood, unwilling to give up a single filbert and yet unable to get them all out at once. Vexed and disappointed he began to cry.\n",
      "\"My boy,\" said his mother, \"be satisfied with half the nuts you have taken and you will easily get your hand out. Then perhaps you may have some more filberts some other time.\"\n",
      "Do not attempt too much at once.\n",
      "\n",
      "\n",
      "\n",
      "Return to theAesoplibrary , or . . . Read the next short story;The Boy And The Nettle\n",
      "Or read more short stories for kids in ourChildren's Library\n",
      "Title: The Boy And The Filberts\n",
      "Text: A Boy was given permission to put his hand into a pitcher to get some filberts. But he took such a g...\n",
      "Fetching story: The Night Came Slowly from /author/kate-chopin/short-story/the-night-came-slowly/...\n",
      "https://americanliterature.com/author/kate-chopin/short-story/the-night-came-slowly/\n",
      "Story text extracted successfully:\n",
      "I am losing my interest in human beings; in the significance of their lives and their actions. Some one has said it is better to study one man than ten books. I want neither books nor men; they make me suffer. Can one of them talk to me like the night – the Summer night? Like the stars or the caressing wind?\n",
      "The night came slowly, softly, as I lay out there under the maple tree. It came creeping, creeping stealthily out of the valley, thinking I did not notice. And the outlines of trees and foliage nearby blended in one black mass and the night came stealing out from them, too, and from the east and west, until the only light was in the sky, filtering through the maple leaves and a star looking down through every cranny.\n",
      "The night is solemn and it means mystery.\n",
      "Human shapes flitted by like intangible things. Some stole up like little mice to peep at me. I did not mind. My whole being was abandoned to the soothing and penetrating charm of the night.\n",
      "The katydids began their slumber song: they are at it yet. How wise they are. They do not chatter like people. They tell me only: “sleep, sleep, sleep.” The wind rippled the maple leaves like little warm love thrills.\n",
      "Why do fools cumber the Earth! It was a man’s voice that broke the necromancer’s spell. A man came to-day with his “Bible Class.” He is detestable with his red cheeks and bold eyes and coarse manner and speech. What does he know of Christ? Shall I ask a young fool who was born yesterday and will die tomorrow to tell me things of Christ? I would rather ask the stars: they have seen him.\n",
      "This story is featured in our collection ofShort-Short Storiesto read when you have five minutes to spare.\n",
      "\n",
      "\n",
      "\n",
      "Return to theKate Chopinlibrary , or . . . Read the next short story;The Recovery\n",
      "Title: The Boy And The Filberts\n",
      "Text: A Boy was given permission to put his hand into a pitcher to get some filberts. But he took such a g...\n",
      "Title: The Night Came Slowly\n",
      "Text: I am losing my interest in human beings; in the significance of their lives and their actions. Some ...\n",
      "Fetching story: One Summer Night from /author/ambrose-bierce/short-story/one-summer-night/...\n",
      "https://americanliterature.com/author/ambrose-bierce/short-story/one-summer-night/\n",
      "Story text extracted successfully:\n",
      "The fact that Henry Armstrong was buried did not seem to him to prove that he was dead: he had always been a hard man to convince. That he really was buried, the testimony of his senses compelled him to admit. His posture -- flat upon his back, with his hands crossed upon his stomach and tied with something that he easily broke without profitably altering the situation -- the strict confinement of his entire person, the black darkness and profound silence, made a body of evidence impossible to controvert and he accepted it without cavil.\n",
      "But dead -- no; he was only very, very ill. He had, withal, the invalid's apathy and did not greatly concern himself about the uncommon fate that had been allotted to him. No philosopher was he -- just a plain, commonplace person gifted, for the time being, with a pathological indifference: the organ that he feared consequences with was torpid. So, with no particular apprehension for his immediate future, he fell asleep and all was peace with Henry Armstrong.\n",
      "But something was going on overhead. It was a dark summer night, shot through with infrequent shimmers of lightning silently firing a cloud lying low in the west and portending a storm. These brief, stammering illuminations brought out with ghastly distinctness the monuments and headstones of the cemetery and seemed to set them dancing. It was not a night in which any credible witness was likely to be straying about a cemetery, so the three men who were there, digging into the grave of Henry Armstrong, felt reasonably secure.\n",
      "Two of them were young students from a medical college a few miles away; the third was a gigantic negro known as Jess. For many years Jess had been employed about the cemetery as a man-of-all-work and it was his favourite pleasantry that he knew 'every soul in the place.' From the nature of what he was now doing it was inferable that the place was not so populous as its register may have shown it to be.\n",
      "Outside the wall, at the part of the grounds farthest from the public road, were a horse and a light wagon, waiting.\n",
      "The work of excavation was not difficult: the earth with which the grave had been loosely filled a few hours before offered little resistance and was soon thrown out. Removal of the casket from its box was less easy, but it was taken out, for it was a perquisite of Jess, who carefully unscrewed the cover and laid it aside, exposing the body in black trousers and white shirt. At that instant the air sprang to flame, a cracking shock of thunder shook the stunned world and Henry Armstrong tranquilly sat up. With inarticulate cries the men fled in terror, each in a different direction. For nothing on earth could two of them have been persuaded to return. But Jess was of another breed.\n",
      "In the grey of the morning the two students, pallid and haggard from anxiety and with the terror of their adventure still beating tumultuously in their blood, met at the medical college.\n",
      "'You saw it?' cried one.\n",
      "'God! yes -- what are we to do?'\n",
      "They went around to the rear of the building, where they saw a horse, attached to a light wagon, hitched to a gatepost near the door of the dissecting-room. Mechanically they entered the room. On a bench in the obscurity sat the negro Jess. He rose, grinning, all eyes and teeth.\n",
      "'I'm waiting for my pay,' he said.\n",
      "Stretched naked on a long table lay the body of Henry Armstrong, the head defiled with blood and clay from a blow with a spade.\n",
      "This story is featured in our collection ofShortShort Storiesto read when you have five minutes to spare, andShort Stories for Middle School II\n",
      "\n",
      "\n",
      "\n",
      "Return to theAmbrose Biercelibrary , or . . . Read the next short story;Parker Adderson, Philosopher\n",
      "Title: The Boy And The Filberts\n",
      "Text: A Boy was given permission to put his hand into a pitcher to get some filberts. But he took such a g...\n",
      "Title: The Night Came Slowly\n",
      "Text: I am losing my interest in human beings; in the significance of their lives and their actions. Some ...\n",
      "Title: One Summer Night\n",
      "Text: The fact that Henry Armstrong was buried did not seem to him to prove that he was dead: he had alway...\n",
      "Fetching story: The Coming of the King from /author/laura-e-richards/short-story/the-coming-of-the-king/...\n",
      "https://americanliterature.com/author/laura-e-richards/short-story/the-coming-of-the-king/\n",
      "Story text extracted successfully:\n",
      "OME children were at play in their play-ground oneday, when a herald rode through the town, blowing a trumpet, and crying aloud, “The King! the King passes by this road to-day. Make ready for the King!”\n",
      "The children stopped their play, and looked at one another.\n",
      "“Did you hear that?” they said. “The King is coming. He may look over the wall and see our playground; who knows? We must put it in order.”\n",
      "The playground was sadly dirty, and in the corners were scraps of paper and broken toys, for these were careless children. But now, one brought a hoe, and another a rake, and a third ran to fetch the wheelbarrow from behind the garden gate. They labored hard, till at length all was clean and tidy.\n",
      "“Now it is clean!” they said; “but we must make it pretty, too, for kings are used to fine things; maybe he would not notice mere cleanness, for he may have it all the time.”\n",
      "Then one brought sweet rushes and strewed them on the ground; and others made garlands of oak leaves and pine tassels and hung them on the walls; and the littlest one pulled marigold buds and threw them all about the playground, “to look like gold,” he said.\n",
      "When all was done the playground was so beautiful that the children stood and looked at it, and clapped their hands with pleasure.\n",
      "“Let us keep it always like this!” said the littlest one; and the others cried, “Yes! yes! that is what we will do.”\n",
      "They waited all day for the coming of the King, but he never came; only, towards sunset, a man with travel-worn clothes, and a kind, tired face passed along the road, and stopped to look over the wall.\n",
      "“What a pleasant place!” said the man. “May I come in and rest, dear children?”\n",
      "The children brought him in gladly, and set him on the seat that they had made out of an old cask. They had covered it with the old red cloak to make it look like a throne, and it made a very good one.\n",
      "“It is our playground!” they said. “We made it pretty for the King, but he did not come, and now we mean to keep it so for ourselves.”\n",
      "“That is good!” said the man.\n",
      "“Because we think pretty and clean is nicer than ugly and dirty!” said another.\n",
      "“That is better!” said the man.\n",
      "“And for tired people to rest in!” said the littlest one.\n",
      "“That is best of all!” said the man.\n",
      "He sat and rested, and looked at the children with such kind eyes that they came about him, and told him all they knew; about the five puppies in the barn, and the thrush’s nest with four blue eggs, and the shore where the gold shells grew; and the man nodded and understood all about it.\n",
      "By and by he asked for a cup of water, and they brought it to him in the best cup, with the gold sprigs on it: then he[14] thanked the children, and rose and went on his way; but before he went he laid his hand on their heads for a moment, and the touch went warm to their hearts.\n",
      "The children stood by the wall and watched the man as he went slowly along. The sun was setting, and the light fell in long slanting rays across the road.\n",
      "“He looks so tired!” said one of the children.\n",
      "“But he was so kind!” said another.\n",
      "“See!” said the littlest one. “How the sun shines on his hair! it looks like a crown of gold.”\n",
      "This story is reminiscent of Oscar Wilde's powerful, spiritual tale,The Selfish Giant, a must-read story for all ages.Featured inPre-K Read-Aloud Stories,Children's Stories, andFeel-Good Stories for Children\n",
      "Featured inPre-K Read-Aloud Stories,Children's Stories, andFeel-Good Stories for Children\n",
      "8.1AddThe Coming of the Kingto your library.Return to theLaura E. Richardslibrary\n",
      ", or . . . Read the next short story;The Golden WindowsOr read more short stories for kids in ourChildren's Library\n",
      "AddThe Coming of the Kingto your library.Return to theLaura E. Richardslibrary\n",
      ", or . . . Read the next short story;The Golden WindowsOr read more short stories for kids in ourChildren's Library\n",
      "Return to theLaura E. Richardslibrary\n",
      ", or . . . Read the next short story;The Golden WindowsOr read more short stories for kids in ourChildren's Library\n",
      "Return to theLaura E. Richardslibrary\n",
      ", or . . . Read the next short story;The Golden Windows\n",
      "Or read more short stories for kids in ourChildren's Library\n",
      "Title: The Boy And The Filberts\n",
      "Text: A Boy was given permission to put his hand into a pitcher to get some filberts. But he took such a g...\n",
      "Title: The Night Came Slowly\n",
      "Text: I am losing my interest in human beings; in the significance of their lives and their actions. Some ...\n",
      "Title: One Summer Night\n",
      "Text: The fact that Henry Armstrong was buried did not seem to him to prove that he was dead: he had alway...\n",
      "Title: The Coming of the King\n",
      "Text: OME children were at play in their play-ground oneday, when a herald rode through the town, blowing ...\n",
      "Original Story:\n",
      " A Boy was given permission to put his hand into a pitcher to get some filberts. But he took such a g\n",
      "\n",
      "Scrambled Story:\n",
      " into other to easily out But his theAesoplibrary , Library them great out have There at mother, stor\n",
      "==================================================\n",
      "Original Story:\n",
      " I am losing my interest in human beings; in the significance of their lives and their actions. Some \n",
      "\n",
      "Scrambled Story:\n",
      " the mystery. like and not looking The to came of neither man he has rippled little is it make peep t\n",
      "==================================================\n",
      "Original Story:\n",
      " The fact that Henry Armstrong was buried did not seem to him to prove that he was dead: he had alway\n",
      "\n",
      "Scrambled Story:\n",
      " -- In a their likely did cloud was the upon been who gifted, place.' them air 'God! which was hard b\n",
      "==================================================\n",
      "Original Story:\n",
      " OME children were at play in their play-ground oneday, when a herald rode through the town, blowing \n",
      "\n",
      "Scrambled Story:\n",
      " Featured the inPre-K our shore like pretty he E. powerful, all better!” the a clapped the Library Ki\n",
      "==================================================\n",
      "A Boy was given permission to put his hand into a pitcher to get some filberts. But he took such a great fistful that he could not draw his hand out again. There he stood, unwilling to give up a single filbert and yet unable to get them all out at once. Vexed and disappointed he began to cry.\n",
      "\"My boy,\" said his mother, \"be satisfied with half the nuts you have taken and you will easily get your hand out. Then perhaps you may have some more filberts some other time.\"\n",
      "Do not attempt too much at once.\n",
      "\n",
      "\n",
      "\n",
      "Return to theAesoplibrary , or . . . Read the next short story;The Boy And The Nettle\n",
      "Or read more short stories for kids in ourChildren's Library\n",
      "into other to easily out But his theAesoplibrary , Library them great out have There at mother, stories The took you was with hand to may his Boy Nettle your not at boy,\" and not \"My a taken stood, once. said up next filbert once. or began and perhaps fistful Vexed you that too single . more half put he ourChildren's filberts satisfied much . Do hand he get give a out. filberts. the cry. to he unable to to all Read Or more . such Return yet short a A for story;The to short and And some you time.\" could have the get \"be hand draw Boy in permission disappointed will pitcher nuts read some get kids he his again. unwilling attempt given some Then\n"
     ]
    }
   ],
   "source": [
    "%run '/home/ec2-user/kaggle/rudolph/dataPrep.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Boy was given permission to put his hand into a pitcher to get some filberts. But he took such a great fistful that he could not draw his hand out again. There he stood, unwilling to give up a single filbert and yet unable to get them all out at once. Vexed and disappointed he began to cry.\n",
      "\"My boy,\" said his mother, \"be satisfied with half the nuts you have taken and you will easily get your hand out. Then perhaps you may have some more filberts some other time.\"\n",
      "Do not attempt too much at once.\n",
      "\n",
      "\n",
      "\n",
      "Return to theAesoplibrary , or . . . Read the next short story;The Boy And The Nettle\n",
      "Or read more short stories for kids in ourChildren's Library\n",
      "into other to easily out But his theAesoplibrary , Library them great out have There at mother, stories The took you was with hand to may his Boy Nettle your not at boy,\" and not \"My a taken stood, once. said up next filbert once. or began and perhaps fistful Vexed you that too single . more half put he ourChildren's filberts satisfied much . Do hand he get give a out. filberts. the cry. to he unable to to all Read Or more . such Return yet short a A for story;The to short and And some you time.\" could have the get \"be hand draw Boy in permission disappointed will pitcher nuts read some get kids he his again. unwilling attempt given some Then\n"
     ]
    }
   ],
   "source": [
    "print(stories[0][0])\n",
    "print(stories[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset: Pairs of original and scrambled sentences \n",
    "\"\"\"\n",
    "sentences = [\n",
    "    (\"I love programming\", \"programming I love\"),\n",
    "    (\"Keras makes deep learning easy\", \"makes easy Keras deep learning\"),\n",
    "    (\"The cat sat on the mat\", \"on the mat The cat sat\"),\n",
    "    (\"Hello world\", \"world Hello\"),\n",
    "    (\"Pine Cone and Pepper Pot ran after her. Jeremy Mouse only stuck his head out of the snow cave. He didn’t want Jack Frost to nip his toes and certainly not his tail! Jack Frost! Jack Frost! cried Tiptoes. We’ve come to pay a visit. Jack Frost landed next to them. He was pale, icy blue, and his arms and fingers were long and glistening. His eyes were sharp, and his wings crackled like breaking ice whenever he moved. He looked so fierce that Pine Cone and Pepper Pot were afraid to open their mouths. It became so chilly they wrapped their beards round their necks and pulled their hats over their ears.\", \"snow he beards only fingers so out their We’ve of and came his sharp, they Cone her. afraid glistening. he Jack long, their It Jack He open his Pot! was to Pot and hats nip after not He necks the ears. Jeremy over Jack next icy Mouse round moved. He tail! was pale, the pay their icy pine wrapped their didn’t heads ice fierce glistening. mouths. breaking over pay Jack next like arms his moved. and certainly! only out was the look Tiptoes over not mouths. and he his to Pepper afraid cave. him Jeremy stuck crackled came blue, pulled ran his round with tail! He them. Mouse after didn’t nip fingers wings their not tail! mouse Pot afraid stuck arms of toes icy Cone was them pay. Pine wings came Pepper their He fierce long his wings Cone hats icy his over and next certainly Jack after ears. glistening. pay tiptoes! like pine ran was icy cave! certainly moved the.\"),\n",
    "    (\"Machine learning is fascinating\", \"fascinating Machine is learning\"),\n",
    "]\n",
    "\"\"\"\n",
    "sentences = stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I love programming', 'programming I love')\n",
      "('A Boy was given permission to put his hand into a pitcher to get some filberts. But he took such a great fistful that he could not draw his hand out again. There he stood, unwilling to give up a single filbert and yet unable to get them all out at once. Vexed and disappointed he began to cry.\\n\"My boy,\" said his mother, \"be satisfied with half the nuts you have taken and you will easily get your hand out. Then perhaps you may have some more filberts some other time.\"\\nDo not attempt too much at once.\\n\\n\\n\\nReturn to theAesoplibrary , or . . . Read the next short story;The Boy And The Nettle\\nOr read more short stories for kids in ourChildren\\'s Library', 'into other to easily out But his theAesoplibrary , Library them great out have There at mother, stories The took you was with hand to may his Boy Nettle your not at boy,\" and not \"My a taken stood, once. said up next filbert once. or began and perhaps fistful Vexed you that too single . more half put he ourChildren\\'s filberts satisfied much . Do hand he get give a out. filberts. the cry. to he unable to to all Read Or more . such Return yet short a A for story;The to short and And some you time.\" could have the get \"be hand draw Boy in permission disappointed will pitcher nuts read some get kids he his again. unwilling attempt given some Then')\n"
     ]
    }
   ],
   "source": [
    "#print(sentences[0])\n",
    "#print(stories[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 8.7604e-04 - loss: 6.7581 - val_accuracy: 0.5756 - val_loss: 6.6518\n",
      "Epoch 2/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 710ms/step - accuracy: 0.3473 - loss: 6.6940 - val_accuracy: 0.5782 - val_loss: 6.4925\n",
      "Epoch 3/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step - accuracy: 0.3574 - loss: 6.5979 - val_accuracy: 0.5782 - val_loss: 6.1995\n",
      "Epoch 4/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step - accuracy: 0.3522 - loss: 6.4228 - val_accuracy: 0.5769 - val_loss: 5.7085\n",
      "Epoch 5/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step - accuracy: 0.3465 - loss: 6.1307 - val_accuracy: 0.5769 - val_loss: 5.1390\n",
      "Epoch 6/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step - accuracy: 0.3460 - loss: 5.7897 - val_accuracy: 0.5769 - val_loss: 4.6398\n",
      "Epoch 7/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step - accuracy: 0.3460 - loss: 5.4843 - val_accuracy: 0.5769 - val_loss: 4.2081\n",
      "Epoch 8/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.3460 - loss: 5.1930 - val_accuracy: 0.5769 - val_loss: 3.8661\n",
      "Epoch 9/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step - accuracy: 0.3460 - loss: 4.9722 - val_accuracy: 0.5769 - val_loss: 3.5827\n",
      "Epoch 10/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708ms/step - accuracy: 0.3460 - loss: 4.7945 - val_accuracy: 0.5769 - val_loss: 3.3655\n",
      "Epoch 11/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 694ms/step - accuracy: 0.3460 - loss: 4.6665 - val_accuracy: 0.5769 - val_loss: 3.2302\n",
      "Epoch 12/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step - accuracy: 0.3460 - loss: 4.5962 - val_accuracy: 0.5769 - val_loss: 3.1636\n",
      "Epoch 13/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step - accuracy: 0.3460 - loss: 4.5601 - val_accuracy: 0.5769 - val_loss: 3.1204\n",
      "Epoch 14/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step - accuracy: 0.3460 - loss: 4.5082 - val_accuracy: 0.5769 - val_loss: 3.0726\n",
      "Epoch 15/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 694ms/step - accuracy: 0.3460 - loss: 4.4205 - val_accuracy: 0.5769 - val_loss: 3.0249\n",
      "Epoch 16/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706ms/step - accuracy: 0.3460 - loss: 4.3186 - val_accuracy: 0.5769 - val_loss: 2.9845\n",
      "Epoch 17/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step - accuracy: 0.3465 - loss: 4.2241 - val_accuracy: 0.5769 - val_loss: 2.9547\n",
      "Epoch 18/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708ms/step - accuracy: 0.3460 - loss: 4.1465 - val_accuracy: 0.5769 - val_loss: 2.9357\n",
      "Epoch 19/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 693ms/step - accuracy: 0.3469 - loss: 4.0862 - val_accuracy: 0.5769 - val_loss: 2.9235\n",
      "Epoch 20/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.3465 - loss: 4.4098 - val_accuracy: 0.5769 - val_loss: 2.9144\n",
      "Epoch 21/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 692ms/step - accuracy: 0.3469 - loss: 4.3895 - val_accuracy: 0.5769 - val_loss: 2.9072\n",
      "Epoch 22/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 694ms/step - accuracy: 0.3460 - loss: 3.9763 - val_accuracy: 0.5769 - val_loss: 2.9013\n",
      "Epoch 23/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step - accuracy: 0.3460 - loss: 3.9521 - val_accuracy: 0.5769 - val_loss: 2.8958\n",
      "Epoch 24/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step - accuracy: 0.3460 - loss: 3.9305 - val_accuracy: 0.5769 - val_loss: 2.8906\n",
      "Epoch 25/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 693ms/step - accuracy: 0.3460 - loss: 3.9105 - val_accuracy: 0.5769 - val_loss: 2.8861\n",
      "Epoch 26/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step - accuracy: 0.3460 - loss: 3.8915 - val_accuracy: 0.5769 - val_loss: 2.8826\n",
      "Epoch 27/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.3460 - loss: 3.8735 - val_accuracy: 0.5769 - val_loss: 2.8801\n",
      "Epoch 28/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step - accuracy: 0.3460 - loss: 3.8563 - val_accuracy: 0.5769 - val_loss: 2.8789\n",
      "Epoch 29/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 691ms/step - accuracy: 0.3460 - loss: 3.8402 - val_accuracy: 0.5821 - val_loss: 2.8790\n",
      "Epoch 30/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 693ms/step - accuracy: 0.3544 - loss: 3.8256 - val_accuracy: 0.6018 - val_loss: 2.8803\n",
      "Epoch 31/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.3872 - loss: 3.8125 - val_accuracy: 0.6032 - val_loss: 2.8826\n",
      "Epoch 32/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 688ms/step - accuracy: 0.3903 - loss: 3.8009 - val_accuracy: 0.6018 - val_loss: 2.8853\n",
      "Epoch 33/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 694ms/step - accuracy: 0.3890 - loss: 3.7909 - val_accuracy: 0.6018 - val_loss: 2.8883\n",
      "Epoch 34/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step - accuracy: 0.3890 - loss: 3.7821 - val_accuracy: 0.6018 - val_loss: 2.8912\n",
      "Epoch 35/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 692ms/step - accuracy: 0.3890 - loss: 3.7740 - val_accuracy: 0.6018 - val_loss: 2.8935\n",
      "Epoch 36/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step - accuracy: 0.3890 - loss: 3.7663 - val_accuracy: 0.6018 - val_loss: 2.8953\n",
      "Epoch 37/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step - accuracy: 0.3890 - loss: 3.7589 - val_accuracy: 0.6032 - val_loss: 2.8961\n",
      "Epoch 38/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step - accuracy: 0.3890 - loss: 3.7517 - val_accuracy: 0.6032 - val_loss: 2.8960\n",
      "Epoch 39/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step - accuracy: 0.3890 - loss: 3.7447 - val_accuracy: 0.6032 - val_loss: 2.8949\n",
      "Epoch 40/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.3890 - loss: 3.7378 - val_accuracy: 0.6032 - val_loss: 2.8931\n",
      "Epoch 41/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step - accuracy: 0.3890 - loss: 3.7314 - val_accuracy: 0.6032 - val_loss: 2.8925\n",
      "Epoch 42/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step - accuracy: 0.3890 - loss: 3.7252 - val_accuracy: 0.6032 - val_loss: 2.8935\n",
      "Epoch 43/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 716ms/step - accuracy: 0.3890 - loss: 3.7190 - val_accuracy: 0.6032 - val_loss: 2.8949\n",
      "Epoch 44/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 693ms/step - accuracy: 0.3890 - loss: 3.7132 - val_accuracy: 0.6032 - val_loss: 2.8965\n",
      "Epoch 45/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step - accuracy: 0.3890 - loss: 3.7077 - val_accuracy: 0.6032 - val_loss: 2.8990\n",
      "Epoch 46/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 694ms/step - accuracy: 0.3890 - loss: 3.7019 - val_accuracy: 0.6032 - val_loss: 2.9026\n",
      "Epoch 47/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step - accuracy: 0.3890 - loss: 3.6958 - val_accuracy: 0.6018 - val_loss: 2.9075\n",
      "Epoch 48/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.3890 - loss: 3.6894 - val_accuracy: 0.6018 - val_loss: 2.9125\n",
      "Epoch 49/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 710ms/step - accuracy: 0.3890 - loss: 3.6826 - val_accuracy: 0.6018 - val_loss: 2.9160\n",
      "Epoch 50/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.3885 - loss: 3.6763 - val_accuracy: 0.6018 - val_loss: 2.9229\n",
      "Epoch 51/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step - accuracy: 0.3885 - loss: 3.6701 - val_accuracy: 0.6018 - val_loss: 2.9312\n",
      "Epoch 52/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 704ms/step - accuracy: 0.3890 - loss: 3.6637 - val_accuracy: 0.6018 - val_loss: 2.9363\n",
      "Epoch 53/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706ms/step - accuracy: 0.3890 - loss: 3.6575 - val_accuracy: 0.6018 - val_loss: 2.9377\n",
      "Epoch 54/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.3890 - loss: 3.6507 - val_accuracy: 0.6018 - val_loss: 2.9370\n",
      "Epoch 55/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.3890 - loss: 3.6436 - val_accuracy: 0.6018 - val_loss: 2.9357\n",
      "Epoch 56/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step - accuracy: 0.3890 - loss: 3.6364 - val_accuracy: 0.6018 - val_loss: 2.9359\n",
      "Epoch 57/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step - accuracy: 0.3890 - loss: 3.6292 - val_accuracy: 0.6018 - val_loss: 2.9421\n",
      "Epoch 58/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.3890 - loss: 3.6215 - val_accuracy: 0.6018 - val_loss: 2.9509\n",
      "Epoch 59/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 694ms/step - accuracy: 0.3890 - loss: 3.6136 - val_accuracy: 0.6018 - val_loss: 2.9571\n",
      "Epoch 60/60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 703ms/step - accuracy: 0.3890 - loss: 3.6057 - val_accuracy: 0.6018 - val_loss: 2.9594\n"
     ]
    }
   ],
   "source": [
    "# Add <start> and <end> tokens to the original sentences\n",
    "original_sentences = [f\"<start> {s[0]} <end>\" for s in sentences]\n",
    "scrambled_sentences = [s[1] for s in sentences]\n",
    "\n",
    "# Tokenize and preprocess sentences\n",
    "tokenizer = Tokenizer(filters=\"\")\n",
    "tokenizer.fit_on_texts(original_sentences + scrambled_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "\n",
    "max_seq_length = max(\n",
    "    max(len(seq.split()) for seq in scrambled_sentences),\n",
    "    max(len(seq.split()) for seq in original_sentences),\n",
    ")\n",
    "\n",
    "# Convert sentences to sequences and pad\n",
    "input_sequences = pad_sequences(tokenizer.texts_to_sequences(scrambled_sentences), maxlen=max_seq_length, padding=\"post\")\n",
    "target_sequences = pad_sequences(tokenizer.texts_to_sequences(original_sentences), maxlen=max_seq_length, padding=\"post\")\n",
    "\n",
    "# Create target sequences for teacher forcing\n",
    "target_input_sequences = target_sequences[:, :-1]\n",
    "target_output_sequences = target_sequences[:, 1:]\n",
    "\n",
    "# Define model parameters\n",
    "embedding_dim = 200\n",
    "latent_dim = 200\n",
    "\n",
    "# Define Encoder\n",
    "encoder_inputs = Input(shape=(max_seq_length,))\n",
    "encoder_embedding = Embedding(vocab_size, embedding_dim)(encoder_inputs)\n",
    "encoder_lstm, state_h, state_c = LSTM(latent_dim, return_state=True)(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Define Decoder\n",
    "decoder_inputs = Input(shape=(max_seq_length - 1,))\n",
    "decoder_embedding = Embedding(vocab_size, embedding_dim)(decoder_inputs)\n",
    "decoder_lstm, _, _ = LSTM(latent_dim, return_sequences=True, return_state=True)(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_outputs = Dense(vocab_size, activation=\"softmax\")(decoder_lstm)\n",
    "\n",
    "# Define model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Prepare data for training\n",
    "decoder_target_output = np.expand_dims(target_output_sequences, -1)\n",
    "X_train, X_test, y_train, y_test, decoder_y_train, decoder_y_test = train_test_split(\n",
    "    input_sequences, target_input_sequences, decoder_target_output, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training the model...\")\n",
    "model.fit(\n",
    "    [X_train, y_train], decoder_y_train,\n",
    "    validation_data=([X_test, y_test], decoder_y_test),\n",
    "    batch_size=50,\n",
    "    epochs=60,\n",
    ")\n",
    "\n",
    "# Define inference models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_lstm_outputs, state_h, state_c = LSTM(latent_dim, return_sequences=True, return_state=True)(\n",
    "    decoder_embedding, initial_state=decoder_states_inputs\n",
    ")\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = Dense(vocab_size, activation=\"softmax\")(decoder_lstm_outputs)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to decode scrambled sentences\n",
    "def decode_sequence(input_seq, scrambled_sentence):\n",
    "    \"\"\"\n",
    "    Decode a scrambled sentence while ensuring the output is a permutation\n",
    "    of the words in the scrambled sentence.\n",
    "    \"\"\"\n",
    "    # Encode the input as state vectors\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Tokenize the scrambled sentence to get word counts\n",
    "    scrambled_words = scrambled_sentence.split()\n",
    "    word_count = Counter(scrambled_words)\n",
    "\n",
    "    # Generate the initial target sequence with the start token\n",
    "    target_seq = np.zeros((1, max_seq_length - 1))\n",
    "    target_seq[0, 0] = tokenizer.word_index[\"<start>\"]\n",
    "\n",
    "    # Initialize variables for decoding\n",
    "    stop_condition = False\n",
    "    decoded_sentence = []\n",
    "\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Create a mask for valid words\n",
    "        valid_word_indices = []\n",
    "        for word, count in word_count.items():\n",
    "            if count > 0:  # Only include words still available in scrambled input\n",
    "                token_index = tokenizer.word_index.get(word, None)\n",
    "                if token_index is not None:\n",
    "                    valid_word_indices.append(token_index)\n",
    "\n",
    "        # Mask out probabilities of invalid words\n",
    "        if valid_word_indices:\n",
    "            mask = np.ones(output_tokens[0, -1, :].shape, dtype=bool)\n",
    "            mask[valid_word_indices] = False  # Only allow valid word indices\n",
    "            output_tokens[0, -1, mask] = 0\n",
    "        else:\n",
    "            # No valid words left, terminate decoding\n",
    "            stop_condition = True\n",
    "            continue\n",
    "\n",
    "        # Resample a valid word\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = tokenizer.index_word.get(sampled_token_index, None)\n",
    "\n",
    "        # Debugging prints\n",
    "        print(f\"Resampled token index: {sampled_token_index}\")\n",
    "        print(f\"Resampled word: {sampled_word}\")\n",
    "\n",
    "        # Stop conditions\n",
    "        if (\n",
    "            sampled_word == \"<end>\"\n",
    "            or len(decoded_sentence) >= max_seq_length - 1\n",
    "            or sum(word_count.values()) == 0  # Stop when all words are used\n",
    "        ):\n",
    "            stop_condition = True\n",
    "        else:\n",
    "            # Add the valid word to the output and decrement its count\n",
    "            if word_count.get(sampled_word, 0) > 0:\n",
    "                decoded_sentence.append(sampled_word)\n",
    "                word_count[sampled_word] -= 1\n",
    "\n",
    "            # Update the target sequence\n",
    "            target_seq = np.zeros((1, max_seq_length - 1))\n",
    "            target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "            # Update states\n",
    "            states_value = [h, c]\n",
    "\n",
    "    return \" \".join(decoded_sentence).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n",
      "Resampled token index: 23\n",
      "Resampled word: on\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
      "Resampled token index: 1\n",
      "Resampled word: the\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "Resampled token index: 1\n",
      "Resampled word: the\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "Resampled token index: 91\n",
      "Resampled word: sat\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "Scrambled: on the mat the cat sat\n",
      "Unscrambled: on the the sat\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "scrambled_input = \"on the mat the cat sat\"\n",
    "input_sequence = pad_sequences(tokenizer.texts_to_sequences([scrambled_input]), maxlen=max_seq_length, padding=\"post\")\n",
    "unscrambled_sentence = decode_sequence(input_sequence, scrambled_input)\n",
    "print(f\"Scrambled: {scrambled_input}\")\n",
    "print(f\"Unscrambled: {unscrambled_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'on': 1, 'the': 1, 'mat': 1, 'The': 1, 'cat': 1, 'sat': 1})\n",
      "['<start> I love programming <end>', '<start> Keras makes deep learning easy <end>', '<start> The cat sat on the mat <end>', '<start> Hello world <end>', '<start> Machine learning is fascinating <end>']\n",
      "['programming I love', 'makes easy Keras deep learning', 'on the mat The cat sat', 'world Hello', 'fascinating Machine is learning']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "scrambled_sentence = \"on the mat The cat sat\"\n",
    "scrambled_words = scrambled_sentence.split()\n",
    "print(Counter(scrambled_words))\n",
    "print(original_sentences)\n",
    "print(scrambled_sentences)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
